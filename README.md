# Case Be Analytic - Pipeline de Mobilidade e Cidadania Operacional (MCO)

Este projeto implementa uma plataforma de dados completa utilizando a **Arquitetura Medallion** (Bronze, Silver, Gold) sobre a plataforma **Databricks** e **AWS**, gerenciada via **Terraform**. O objetivo √© extrair, tratar e analisar dados de mobilidade urbana (passageiros por linha) de Belo Horizonte.

---

## üèóÔ∏è Arquitetura do Projeto

O projeto segue as melhores pr√°ticas de Data Engineering moderno:

1.  **Ingest√£o (Bronze)**: Extra√ß√£o de dados crus via API/HTTP e armazenamento em formato Delta.
2.  **Refinamento (Silver)**: Limpeza, normaliza√ß√£o e valida√ß√£o de qualidade de dados.
3.  **An√°lise (Gold)**: Agrega√ß√µes de neg√≥cio e modelagem Star Schema (Fato e Dimens√µes).
4.  **Infraestrutura como C√≥digo (IaC)**: Provisionamento completo via Terraform.
5.  **Governan√ßa**: Unity Catalog para controle de acesso e linhagem de dados.

### Diagrama de Fluxo
```mermaid
graph LR
    subgraph "Fontes Externas"
        CKAN[Portal CKAN PBH]
    end

    subgraph "Lakehouse (S3 + Databricks)"
        Bronze[Camada Bronze: mco_raw] --> Silver[Camada Prata: mco_clean]
        Silver --> Gold[Camada Ouro: fact_passageiros]
        Silver --> GoldDims[Dimens√µes: dim_linha, dim_data]
    end

    subgraph "IaC & CI/CD"
        TF[Terraform / AWS] --> UC[Unity Catalog]
        GH[GitHub Actions] --> Lint[Linting / Black]
        GH --> Deploy[Deploy Databricks Jobs]
    end

    CKAN -- Python Requests --> Bronze
```

---

## üöÄ Passo a Passo e Racioc√≠nio T√©cnico

### 1. Camada Bronze (Extra√ß√£o)
*   **Script**: `scraping/mco_extractor.py`
*   **A√ß√£o**: Download de CSVs mensais do portal da PBH.
*   **Por que?**: Optamos por uma extra√ß√£o via script Spark para permitir escalabilidade futura. Adicionamos metadados t√©cnicos (`_ingestion_timestamp`, `_source_url`) para garantir a rastreabilidade (linhagem) de cada registro.
*   **Destaque**: Grava√ß√£o direta via Unity Catalog External Locations, garantindo que o dado chegue no bucket S3 correto sem manipula√ß√£o manual.

### 2. Camada Silver (Refinamento)
*   **Script**: `pipelines/silver_refinement.py`
*   **A√ß√£o**: Deduplica√ß√£o por `(LINHA, DATA, HORA)`, tratamento de tipos (cast) e filtros de qualidade.
*   **Por que?**: Dados brutos costumam vir com fuso hor√°rio misto ou formatos de n√∫mero inconsistentes (string vs int). O refinamento garante que a camada Gold consuma dados confi√°veis.
*   **Inova√ß√£o**: Implementa√ß√£o de um "Alert Threshold". Se mais de 10% dos dados forem descartados na limpeza, o processo emite um alerta para auditoria humana.

### 3. Camada Gold (Business)
*   **Script**: `pipelines/gold_aggregations.py`
*   **A√ß√£o**: Cria√ß√£o da `fact_passageiros` e dimens√µes `dim_linha` e `dim_data`.
*   **Por que?**: Modelamos em Star Schema para facilitar a conex√£o com ferramentas de BI (PowerBI/Tableau).
*   **Performance**: Aplicamos `ZORDER` na coluna `LINHA` e particionamento por `DATA`. Isso reduz dramaticamente o custo de processamento e o tempo de resposta em consultas filtradas por linha de √¥nibus.

### 4. Infraestrutura e CI/CD
*   **Ferramentas**: Terraform + AWS + GitHub Actions.
*   **Regi√£o**: `us-east-2` (Ohio).
*   **Racioc√≠nio**: O uso de Terraform garante que o ambiente de Dev e Prod sejam id√™nticos. O CI/CD automatiza a publica√ß√£o dos Jobs no Databricks assim que o c√≥digo √© aprovado no Master, evitando erros de deploy manual.
*   **Otimiza√ß√£o de Custos**:
    *   Uso de **Spot Instances** (`SPOT_WITH_FALLBACK`) para os clusters de processamento, reduzindo custos em at√© 70%.
    *   **Autotermination** configurado para 20 minutos de inatividade.
    *   Clusters **Single Node** para tarefas leves, evitando overhead de gerenciamento de m√∫ltiplos workers.

---

## üõ†Ô∏è Desafios Encontrados & Bugs Conhecidos

Mesmo com a entrega funcional, existem pontos de aten√ß√£o fruto de limita√ß√µes do ambiente de desenvolvimento:

*   **Inst√¢ncias AWS m5.xlarge**: Foi necess√°rio configurar manualmente via Terraform o anexo de volumes EBS, pois o Databricks exige armazenamento local para estas fam√≠lias de inst√¢ncias em determinadas regi√µes.
*   **State do Terraform**: Devido ao uso de reposit√≥rios compartilhados, ocorreram problemas de "stale plan". Corrigido com automa√ß√£o de limpeza de estado no GitHub Actions.
*   **Unity Catalog Permissions**: A configura√ß√£o de External Locations exigiu ajustes finos nas Trust Relationships do IAM roles da AWS para permitir que o Databricks escreva nos buckets S3 selecionados.

---

## üìà Melhorias Futuras
- [ ] Implementar processamento incremental (Incremental Loading) usando Delta Live Tables.
- [ ] Adicionar testes de contrato de dados (Great Expectations).
- [ ] Dashboard de Monitoramento de Qualidade de Dados (DQM).

---
**Autor:** [Seu Nome/GitHub]
**Projeto:** Case T√©cnico Be Analytic
